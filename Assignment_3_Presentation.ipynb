{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "268d3f1c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Please don't change this cell\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd47ca13",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please don't change this cell\n",
    "df = pd.read_csv('ml-100k/u.data', names=['user_id', 'item_id', 'rating', 'timestamp'], sep='\\t')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde8fc65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assignment 3 – Presentation – Asher Elazary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3063a26d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 users\n",
      "1682 items\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "print(str(n_users) + ' users')\n",
    "print(str(n_items) + ' items')\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state = 10)\n",
    "train_df, test_df\n",
    "\n",
    "# Training Dataset\n",
    "train_ds = np.zeros((n_users, n_items))\n",
    "for row in train_df.itertuples():\n",
    "    train_ds[row[1]-1, row[2]-1] = row[3]\n",
    "\n",
    "# Testing Dataset\n",
    "test_ds = np.zeros((n_users, n_items))\n",
    "for row in test_df.itertuples():\n",
    "    test_ds[row[1]-1, row[2]-1] = row[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa0105",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 3.1 – Notation\n",
    "\n",
    "$u$ – the (incomplete) set of users ratings (array).\n",
    "\n",
    "$u_i$ – the rating of this user gives to item $i$.\n",
    "\n",
    "$S(u)$ – \"The subset of the set of items consisting of all those items which are rated in u\"\n",
    "\n",
    "$\\chi$ – the set of all evaluations in the training set.\n",
    "\n",
    "$card(S)$ – the number of elements in a set $S$.\n",
    "\n",
    "$S_i(\\chi)$ – the set of all evaluations $u \\in\\chi$ such that they contain item $i \\space(i \\in S(u))$ – i.e. return the set of users who have rated item $i$ (the movie).\n",
    "\n",
    "\n",
    "$P(u)$ – \"represent a vector where each component is the prediction corresponding to one item: predictions depend implicitly on the training set $\\chi$.\"\n",
    "\n",
    "$u \\in S_{j,i}(\\chi)$ – Given a training set $\\chi$, and any two items $j$ and $i$ with ratings $u_j$ and $u_i$ respectively in some user evaluation $u$\n",
    "\n",
    "$card(S_{j,i}(\\chi))$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb2d405",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$R_j = \\{ i|i \\in S(u), i \\neq j, card(S_{j,i}(\\chi)) > 0\\}$$\n",
    "\n",
    "Rj is the set of items i that satisfy the following conditions:\n",
    "\n",
    "* i is an element of the set S(u), meaning that i appears in some evaluation u.\n",
    "* i is not equal to j, ensuring that the item i is different from the specific item j.\n",
    "* The cardinality of the set Sj,i(χ) is greater than 0, indicating that there are evaluations in χ that contain both items j and i.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b09e52a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#use njit calculations\n",
    "from numba import jit, njit\n",
    "\n",
    "# Please don't change this cell\n",
    "EPSILON = 1e-9\n",
    "\n",
    "def evaluate(test_ds, predicted_ds):\n",
    "    '''\n",
    "    Function for evaluating on MAE and RMSE\n",
    "    '''\n",
    "    # MAE\n",
    "    mask_test_ds = test_ds > 0\n",
    "    MAE = np.sum(np.abs(test_ds[mask_test_ds] - predicted_ds[mask_test_ds])) / np.sum(mask_test_ds.astype(np.float32))\n",
    "\n",
    "    # RMSE\n",
    "    RMSE = np.sqrt(np.sum(np.square(test_ds[mask_test_ds] - predicted_ds[mask_test_ds])) / np.sum(mask_test_ds.astype(np.float32)))\n",
    "\n",
    "    return MAE, RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107285c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Slope One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca8c5926",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def make_pairwise_calcs(x):\n",
    "    #zero arrays to build matrix for number of items \n",
    "    dev_arr = np.zeros((n_items, n_items))\n",
    "    freq_arr = np.zeros((n_items, n_items))\n",
    "    \n",
    "    #for each item pair, calculate the average deviation and store in matrix\n",
    "    for j in range(n_items):\n",
    "        #get u rows containing j items\n",
    "        j_in_u = x[:, j] != 0      \n",
    "        if(np.any(j_in_u)):\n",
    "            for i in range(j + 1, n_items):\n",
    "                #get u rows containing i items\n",
    "                i_in_u = x[:, i] != 0\n",
    "                #get the boolean intersection mask for j and i items\n",
    "                intersections = np.logical_and(j_in_u, i_in_u)\n",
    "                #get the sets\n",
    "                u_x_ji = x[intersections]\n",
    "                #get the number of sets\n",
    "                card_ji = u_x_ji.shape[0]\n",
    "                #if co-rated items exist...\n",
    "                if(card_ji > 0):\n",
    "                    #calculate the avg deviation between the pairs\n",
    "                    dev = np.sum((u_x_ji[:, j] - u_x_ji[:, i]) / card_ji)\n",
    "                    #utilise matrix similarity\n",
    "                    dev_arr[j, i] = dev\n",
    "                    dev_arr[i, j] = -dev\n",
    "                    freq_arr[j, i] = card_ji\n",
    "                    freq_arr[i, j] = card_ji\n",
    "                    \n",
    "    return dev_arr, freq_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e93a109",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def predict_s1(u, j , x , dev_arr, freq_arr):    \n",
    "    #all rated item indices for user\n",
    "    u_indices = np.where(x[u])[0]\n",
    "    #item indices not including j\n",
    "    i_neq_j = u_indices != j\n",
    "    u_indices = u_indices[i_neq_j] \n",
    "    #reset variables\n",
    "    pre_ji = 0\n",
    "    #for each item in the user's ratings\n",
    "    for i in u_indices:\n",
    "        #lookup the precomputed co-rating frequency\n",
    "        card_ji = freq_arr[j, i]\n",
    "        #if the co-rated item pair exists\n",
    "        if(card_ji > 0):\n",
    "            #lookup the average deviation per item pair\n",
    "            dev_ji = dev_arr[j, i]\n",
    "            #user prediction for one item pair average deviation + the user rating i\n",
    "            pre_ji += ((dev_ji + x[u, i]))\n",
    "\n",
    "    #item prediction is average of all predictions (in reciprocal form)\n",
    "    return pre_ji * (1/u_indices.size)\n",
    "\n",
    "def make_predictions_s1(X, y):\n",
    "    pre_arr = np.copy(X)\n",
    "    y_pre_indices = np.argwhere(y)\n",
    "    \n",
    "    dev_arr, freq_arr = make_pairwise_calcs(X)\n",
    "\n",
    "    for this_pre in y_pre_indices:\n",
    "        u = this_pre[0]\n",
    "        j = this_pre[1]  \n",
    "        pre_arr[u][j] = predict_s1(u, j, X, dev_arr, freq_arr)\n",
    "\n",
    "    return pre_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00d7c2bc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7638557167602313, 0.9803994548163598)\n",
      "CPU times: user 22.3 s, sys: 1 s, total: 23.3 s\n",
      "Wall time: 26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pre_arr_s1 = make_predictions_s1(train_ds, test_ds)\n",
    "print(evaluate(test_ds, pre_arr_s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f449431",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Weighted Slope One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa241751",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#---WS1---#\n",
    "@njit\n",
    "def predict_ws1(u, j , x , dev_arr, freq_arr):\n",
    "    \n",
    "    u_indices = np.where(x[u])[0]\n",
    "    i_neq_j = u_indices != j\n",
    "    u_indices = u_indices[i_neq_j]\n",
    "    \n",
    "    #reset variables\n",
    "    weighted_pre_ji = 0\n",
    "    weighted_users = 0\n",
    "    \n",
    "    for i in u_indices:     \n",
    "        card_ji = freq_arr[j, i]      \n",
    "        if(card_ji > 0):\n",
    "            dev_ji = dev_arr[j, i]\n",
    "            #this time, we weight the prediction based on the amount of users that have rated item-pair\n",
    "            weighted_pre_ji += ((dev_ji + x[u, i]) * card_ji)\n",
    "            weighted_users += card_ji\n",
    "    \n",
    "    if(weighted_users > 0):\n",
    "        #returned prediction for item is average of all predictions weighted by the users per item pair prediction\n",
    "        return weighted_pre_ji / weighted_users\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def make_predictions_ws1(X, y):\n",
    "    pre_arr = np.copy(X)\n",
    "    y_pre_indices = np.argwhere(y)\n",
    "    \n",
    "    dev_arr, freq_arr = make_pairwise_calcs(X)\n",
    "\n",
    "    for this_pre in y_pre_indices:\n",
    "        u = this_pre[0]\n",
    "        j = this_pre[1]  \n",
    "        pre_arr[u][j] = predict_ws1(u, j, X, dev_arr, freq_arr)\n",
    "\n",
    "    return pre_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08ce6961",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.744669520796919, 0.9533358060475977)\n",
      "CPU times: user 21.9 s, sys: 953 ms, total: 22.8 s\n",
      "Wall time: 25.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pre_arr_ws1 = make_predictions_ws1(train_ds, test_ds)\n",
    "print(evaluate(test_ds, pre_arr_ws1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab13545f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Centred Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "659d9d8f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def make_centred_cosine(x):\n",
    "\n",
    "    def centred_cosine_similarity(a,b):\n",
    "        #calculate the mean of each set (ignore 0 values)\n",
    "        a_mean = np.sum(a) / np.count_nonzero(a)\n",
    "        b_mean = np.sum(b) / np.count_nonzero(b)\n",
    "        #subtract the mean of each set from itself (ignore 0 values)\n",
    "        #normalise the magnitude of the vectors\n",
    "        a = np.where(a > 0, a - a_mean, a)\n",
    "        a = np.where(b > 0, a - b_mean, b)\n",
    "        \n",
    "        #calculate the dot product between sets\n",
    "        this_dot = np.sum((a*b))\n",
    "        #calculate the magnitude for each set\n",
    "        mag_a = np.sqrt(np.sum(np.square(a)))\n",
    "        mag_b = np.sqrt(np.sum(np.square(b)))\n",
    "        this_mag = mag_a * mag_b\n",
    "        #get the cosine angle, representing similarity between sets\n",
    "        return (this_dot / this_mag)\n",
    "    \n",
    "    #calculate the symmetrical matrix of cosine similarities\n",
    "    u_sim = np.ones((n_users, n_users))\n",
    "    for u0 in range(n_users):\n",
    "        for u1 in range(u0 + 1, n_users):\n",
    "            this_sim = centred_cosine_similarity(x[u0], x[u1])\n",
    "            #write to symetrical coordinates\n",
    "            u_sim[u0, u1] =  this_sim\n",
    "            u_sim[u1, u0] =  this_sim\n",
    "            \n",
    "    return u_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97831a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modified Weighted Slope One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c321d2a4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def predict_ws1_modified(u_selected, j, x, u_sim, LAMBDA):\n",
    "    \n",
    "    #all rated item indices for user\n",
    "    u_selected_indices = np.where(x[u_selected])[0]\n",
    "    #reinitalise accumulators\n",
    "    weighted_dev = 0\n",
    "    weighted_users = 0\n",
    "\n",
    "    #for each item in the user array\n",
    "    for i in u_selected_indices:\n",
    "    #if i == j, nex iteration\n",
    "        if(i == j):\n",
    "            continue\n",
    "        \n",
    "        #get u rows containing j items\n",
    "        j_in_u = x[:, j] != 0\n",
    "        #get u rows containing i items\n",
    "        i_in_u = x[:, i] != 0\n",
    "        #get the boolean intersection mask for j and i items\n",
    "        intersections = np.logical_and(j_in_u, i_in_u)\n",
    "        #get the indices\n",
    "        u_set_indices = np.nonzero(intersections)[0]\n",
    "        #get the number of users for co-rated sets\n",
    "        card_ji = u_set_indices.size\n",
    "        \n",
    "        #if co-rating exists for item pair\n",
    "        if(card_ji > 0):\n",
    "\n",
    "            #deviation between item pairs (as vector)\n",
    "            dev = x[u_set_indices, j] - x[u_set_indices, i]\n",
    "            #calculate the average deviation between item pairs\n",
    "            avg_dev = np.sum(dev / card_ji)            \n",
    "\n",
    "            #remove selected user from users of co-rated items\n",
    "            u_set_indices = u_set_indices[u_set_indices != u_selected]\n",
    "            #get all cosine similarities between selected users and users of co-rated items\n",
    "            exp_cosine_sim = u_sim[u_set_indices, u_selected]\n",
    "            #calculate the average deviation between item pairs, weighted by the user similarity between co-rated items\n",
    "            exp_cosine_dev = np.sum(exp_cosine_sim * dev)\n",
    "            exp_cosine_users = np.sum(card_ji * exp_cosine_sim)\n",
    "            avg_user_sim = exp_cosine_dev / exp_cosine_users            \n",
    "            #interpolate between the two averages\n",
    "            dev_ji = (LAMBDA * avg_dev) + ((1-LAMBDA)*(avg_user_sim))           \n",
    "            #user prediction for one item pair is the interpolation between the two deviation functions, \n",
    "            #averaged for every i j pair\n",
    "            weighted_dev += ((dev_ji + x[u_selected, i]) * card_ji)\n",
    "            weighted_users += card_ji\n",
    "    \n",
    "    if(weighted_users > 0):\n",
    "        return weighted_dev / weighted_users\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "324f6a9b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def make_predictions_ws1_modified(X, y):\n",
    "    #make a copy of the train dataset\n",
    "    pre_arr = np.copy(X)\n",
    "    #identify rated indices of the test dataset\n",
    "    y_pre_indices = np.argwhere(y)\n",
    "    \n",
    "    #calculate the user similarity matrix. Apply exp2 function to emphasise stronger similarities and minimise weaker similarites\n",
    "    u_sim = np.exp2(make_centred_cosine(X))\n",
    "    \n",
    "    #predict the ratings for these indices in the train dataset\n",
    "    for this_pre in y_pre_indices:\n",
    "        this_u = this_pre[0]\n",
    "        this_j = this_pre[1]  \n",
    "        pre_arr[this_u][this_j] = predict_ws1_modified(this_u, this_j, X, u_sim, 0)\n",
    "\n",
    "    return pre_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d9a8cc0",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8106610636194549, 1.028542049922151)\n",
      "CPU times: user 1min 12s, sys: 1.54 s, total: 1min 13s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pre_arr_ws1_modified = make_predictions_ws1_modified(train_ds, test_ds)\n",
    "print(evaluate(test_ds, pre_arr_ws1_modified))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd4257",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e08d8e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "‘Cosine similarity’ 2023, Wikipedia.\n",
    "\n",
    "Lemire, D & Maclachlan, A 2008, ‘Slope One Predictors for Online Rating-Based Collaborative Filtering’,.\n",
    "\n",
    "Ren, Y 2023, ‘Practical Data Science with Python - COSC 2670/2738 - Assignment 3’,.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "rise": {
   "height": "200%",
   "width": "200%"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
